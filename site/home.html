<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>ImageNet-X: ImageNet-X</title><meta property="og:title" content="ImageNet-X"/><meta property="og:site_name" content="ImageNet-X"/><meta property="og:description" content=""/><meta property="og:type" content="website"/><meta property="og:image" content="/imagenetx/site/assets/thumbnail.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="next-head-count" content="9"/><link rel="preload" href="/imagenetx/site/_next/static/css/0aceb538260b58d7.css" as="style"/><link rel="stylesheet" href="/imagenetx/site/_next/static/css/0aceb538260b58d7.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/imagenetx/site/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/imagenetx/site/_next/static/chunks/webpack-667bf6c97ed38451.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/main-8490ef64073898b2.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/pages/_app-a74e9fe927d31b56.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/78e521c3-4ade3a605555334b.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/301-2088252f6a6ca1a0.js" defer=""></script><script src="/imagenetx/site/_next/static/chunks/pages/%5Bid%5D-0752b23dd29e9358.js" defer=""></script><script src="/imagenetx/site/_next/static/wfzXPFXPAwdwO_5XoQBa8/_buildManifest.js" defer=""></script><script src="/imagenetx/site/_next/static/wfzXPFXPAwdwO_5XoQBa8/_ssgManifest.js" defer=""></script><script src="/imagenetx/site/_next/static/wfzXPFXPAwdwO_5XoQBa8/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div><nav class="bg-white md:shadow"><div class="max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex"><div class="flex-shrink-0 flex items-center cursor-pointer"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2732%27%20height=%2732%27/%3e"/></span><img alt="logo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="logo" srcSet="/imagenetx/site/assets/logo.svg 1x, /imagenetx/site/assets/logo.svg 2x" src="/imagenetx/site/assets/logo.svg" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></div><div class="hidden sm:ml-6 sm:flex sm:space-x-8"><a class="text-indigo-700 hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/imagenetx/site/home">ImageNet-X</a><a class=" hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/imagenetx/site/dataset">Dataset</a><a class=" hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/imagenetx/site/paper">Paper</a></div></div><div class="hidden sm:ml-6 sm:flex sm:space-x-8"><a href="https://github.com/facebookresearch/imagenetx/" class="text-gray-900 hover:text-indigo-500 ml-6 inline-flex items-center px-1 pt-1 text-sm font-semibold" target="_blank">Github<!-- --> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></div><div class="-mr-2 flex items-center sm:hidden"><button class="inline-flex items-center justify-center p-2 rounded-md text-gray-900" id="headlessui-disclosure-button-undefined" type="button" aria-expanded="false"><span class="sr-only">Open main menu</span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="block h-6 w-6" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 4h18v2H3V4zm0 7h18v2H3v-2zm0 7h18v2H3v-2z"></path></g></svg></button></div></div></div></nav><div class="prose prose-starter"><div class="comp_content flex w-screen justify-center self-stretch bg-indigo-200 text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-20 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h1>ImageNet-X</h1><h3>Understanding model mistakes with human annotations of ImageNet.</h3><p><strong>ImageNet-X</strong> is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and
a random subset of 12,000 training samples.</p><span class="comp_button inline-block" style="margin-right:10px"><a href="/home" target="_blank" rel="noopener" class="flex rounded-md no-underline items-center gap-2 justify-center font-semibold text-white hover:text-blue-600 hover:shadow-md bg-blue-600 hover:bg-white px-6 py-4 text-md"><p>Paper</p> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></span><span class="comp_button inline-block" style="margin-right:10px"><a href="https://github.com/facebookresearch/imagenetx/" target="_blank" rel="noopener" class="flex rounded-md no-underline items-center gap-2 justify-center font-semibold text-white hover:text-blue-600 hover:shadow-md bg-blue-600 hover:bg-white px-6 py-4 text-md"><p>GitHub</p> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></span><span class="comp_button inline-block"><a href="https://colab.research.google.com/drive/1snegwqRKjqjwBEyq0-YY9_NOmTai8hT_?usp=sharing" target="_blank" rel="noopener" class="flex rounded-md no-underline items-center gap-2 justify-center font-semibold text-white hover:text-blue-600 hover:shadow-md bg-blue-600 hover:bg-white px-6 py-4 text-md"><p>Colab Notebook</p> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></span></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_gallery flex w-screen justify-center self-stretch bg-indigo-200 text-gray-700"><div class="max-w-screen-xl flex flex-1 flex-row items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-20"><div class="flex flex-1 flex-grow-4 self-start max-w-none "><div class="p-1 aspect-auto"><div class="relative"><img src="/imagenetx/site/assets/images/landing.png" class="comp_image object-cover w-full h-full m-0"/></div></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-20 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700">ImageNet-X can be used to surface mistake types for any ImageNet classifier</div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Using ImageNet-X</h2><ol>
<li>Install imagenet-x package</li>
</ol><pre><code>pip install imagenet-x
</code></pre><ol start="2">
<li>Load annotations</li>
</ol><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> imagenet_x <span class="token keyword">import</span> load_annotations

annots <span class="token operator">=</span> load_annotations<span class="token punctuation">(</span>partition<span class="token operator">=</span><span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
</code></pre><p>See this <a href="https://colab.research.google.com/drive/1snegwqRKjqjwBEyq0-YY9_NOmTai8hT_?usp=sharing">colab</a> for a step by step notebook loading annotations and evaluating new models.</p><p>For advanced usage, see the <a href="https://github.com/facebookresearch/imagenetx/">README</a>.</p></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Research abstract</h2><p>Deep learning vision systems are widely deployed across applications where reliability is critical.
However, even today’s best models can fail to recognize an object when its pose, lighting, or background varies.
While existing benchmarks surface examples that are challenging for models, they do not explain why such mistakes arise.</p><p>To address this need, we introduce ImageNet-X–a set of sixteen human annotations of factors such as pose, background, or lighting for the entire ImageNet1k validation set as well as a random subset of 12k training images.
Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model’s
(1) architecture – e.g. transformer vs. convolutional –, (2) learning paradigm – e.g. supervised vs. self-supervised –, and (3) training procedures – e.g. data augmentation.</p><p>Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories.
We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors.
For example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose.
Together, these insights suggests that to advance the robustness of modern vision models, future research should focus on collecting additional diverse data and understanding data augmentation schemes.
Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems make</p><span class="comp_button inline-block"><a href="/home" target="_blank" rel="noopener" class="flex rounded-md no-underline items-center gap-2 justify-center font-semibold text-white hover:text-blue-600 hover:shadow-md bg-blue-600 hover:bg-white px-6 py-4 text-md"><p>Read the Paper</p> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></span></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_footer flex w-screen justify-center self-stretch bg-slate-200 text-undefined"><div class="flex flex-1 flex-row box-border max-w-screen-xl items-start justify-start px-5 md:px-20 xl:px-10 py-20 "><div class="hidden md:flex flex-1 flex-col mr-4 pt-2"></div><div class="flex flex-1 box-border pr-2 item-start justify-start text-sm"><div><p>ImageNet-X</p></div></div><div class="flex flex-1 box-border pr-2 item-start justify-start text-sm"><div><p><a href="https://github.com/fairinternal/image-variation-categories">Github repo</a></p></div></div><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 "></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"home","mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      h1: \"h1\",\n      h3: \"h3\",\n      p: \"p\",\n      strong: \"strong\",\n      h2: \"h2\",\n      ol: \"ol\",\n      li: \"li\",\n      pre: \"pre\",\n      code: \"code\",\n      span: \"span\",\n      a: \"a\"\n    }, _provideComponents(), props.components), {Content, Button, Gallery, Image, Footer} = _components;\n    if (!Button) _missingMdxReference(\"Button\", true);\n    if (!Content) _missingMdxReference(\"Content\", true);\n    if (!Footer) _missingMdxReference(\"Footer\", true);\n    if (!Gallery) _missingMdxReference(\"Gallery\", true);\n    if (!Image) _missingMdxReference(\"Image\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsxs(Content, {\n        color: \"indigo-200\",\n        spaceTop: true,\n        spaceBottom: true,\n        children: [_jsx(_components.h1, {\n          children: \"ImageNet-X\"\n        }), _jsx(_components.h3, {\n          children: \"Understanding model mistakes with human annotations of ImageNet.\"\n        }), _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"ImageNet-X\"\n          }), \" is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and\\na random subset of 12,000 training samples.\"]\n        }), _jsx(Button, {\n          url: \"/home\",\n          color: \"blue-600\",\n          openNew: true,\n          style: {\n            marginRight: '10px'\n          },\n          children: _jsx(_components.p, {\n            children: \"Paper\"\n          })\n        }), _jsx(Button, {\n          url: \"https://github.com/facebookresearch/imagenetx/\",\n          color: \"blue-600\",\n          openNew: true,\n          style: {\n            marginRight: '10px'\n          },\n          children: _jsx(_components.p, {\n            children: \"GitHub\"\n          })\n        }), _jsx(Button, {\n          url: \"https://colab.research.google.com/drive/1snegwqRKjqjwBEyq0-YY9_NOmTai8hT_?usp=sharing\",\n          color: \"blue-600\",\n          openNew: true,\n          children: _jsx(_components.p, {\n            children: \"Colab Notebook\"\n          })\n        })]\n      }), \"\\n\", _jsx(Gallery, {\n        color: \"indigo-200\",\n        fullWidth: true,\n        spaceBottom: true,\n        children: _jsx(Image, {\n          url: \"/assets/images/landing.png\"\n        })\n      }), \"\\n\", _jsxs(Content, {\n        noteLeft: \"ImageNet-X can be used to surface mistake types for any ImageNet classifier\",\n        spaceTop: true,\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Using ImageNet-X\"\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Install imagenet-x package\"\n          }), \"\\n\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            children: \"pip install imagenet-x\\n\"\n          })\n        }), _jsxs(_components.ol, {\n          start: \"2\",\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Load annotations\"\n          }), \"\\n\"]\n        }), _jsx(_components.pre, {\n          className: \"language-python\",\n          children: _jsxs(_components.code, {\n            className: \"language-python\",\n            children: [_jsx(_components.span, {\n              className: \"token keyword\",\n              children: \"from\"\n            }), \" imagenet_x \", _jsx(_components.span, {\n              className: \"token keyword\",\n              children: \"import\"\n            }), \" load_annotations\\n\\nannots \", _jsx(_components.span, {\n              className: \"token operator\",\n              children: \"=\"\n            }), \" load_annotations\", _jsx(_components.span, {\n              className: \"token punctuation\",\n              children: \"(\"\n            }), \"partition\", _jsx(_components.span, {\n              className: \"token operator\",\n              children: \"=\"\n            }), _jsx(_components.span, {\n              className: \"token string\",\n              children: \"\\\"val\\\"\"\n            }), _jsx(_components.span, {\n              className: \"token punctuation\",\n              children: \")\"\n            }), \"\\n\"]\n          })\n        }), _jsxs(_components.p, {\n          children: [\"See this \", _jsx(_components.a, {\n            href: \"https://colab.research.google.com/drive/1snegwqRKjqjwBEyq0-YY9_NOmTai8hT_?usp=sharing\",\n            children: \"colab\"\n          }), \" for a step by step notebook loading annotations and evaluating new models.\"]\n        }), _jsxs(_components.p, {\n          children: [\"For advanced usage, see the \", _jsx(_components.a, {\n            href: \"https://github.com/facebookresearch/imagenetx/\",\n            children: \"README\"\n          }), \".\"]\n        })]\n      }), \"\\n\", _jsxs(Content, {\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Research abstract\"\n        }), _jsx(_components.p, {\n          children: \"Deep learning vision systems are widely deployed across applications where reliability is critical.\\nHowever, even today’s best models can fail to recognize an object when its pose, lighting, or background varies.\\nWhile existing benchmarks surface examples that are challenging for models, they do not explain why such mistakes arise.\"\n        }), _jsx(_components.p, {\n          children: \"To address this need, we introduce ImageNet-X–a set of sixteen human annotations of factors such as pose, background, or lighting for the entire ImageNet1k validation set as well as a random subset of 12k training images.\\nEquipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model’s\\n(1) architecture – e.g. transformer vs. convolutional –, (2) learning paradigm – e.g. supervised vs. self-supervised –, and (3) training procedures – e.g. data augmentation.\"\n        }), _jsx(_components.p, {\n          children: \"Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories.\\nWe also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors.\\nFor example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose.\\nTogether, these insights suggests that to advance the robustness of modern vision models, future research should focus on collecting additional diverse data and understanding data augmentation schemes.\\nAlong with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems make\"\n        }), _jsx(Button, {\n          url: \"/home\",\n          color: \"blue-600\",\n          openNew: true,\n          children: _jsx(_components.p, {\n            children: \"Read the Paper\"\n          })\n        })]\n      }), \"\\n\", _jsxs(Footer, {\n        columns: true,\n        children: [_jsx(\"div\", {\n          children: _jsx(_components.p, {\n            children: \"ImageNet-X\"\n          })\n        }), _jsx(\"div\", {\n          children: _jsx(_components.p, {\n            children: _jsx(_components.a, {\n              href: \"https://github.com/fairinternal/image-variation-categories\",\n              children: \"Github repo\"\n            })\n          })\n        })]\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"currentMetaData":{"title":"ImageNet-X","order":0,"id":"home"},"allMetaData":[{"id":"home","title":"ImageNet-X","order":0},{"id":"dataset","title":"Dataset","order":1},{"id":"paper","title":"Paper","order":2}]},"__N_SSG":true},"page":"/[id]","query":{"id":"home"},"buildId":"wfzXPFXPAwdwO_5XoQBa8","assetPrefix":"/imagenetx/site","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>