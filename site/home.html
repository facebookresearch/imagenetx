<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>ImageNet-X: ImageNet-X</title><meta property="og:title" content="ImageNet-X"/><meta property="og:site_name" content="ImageNet-X"/><meta property="og:description" content=""/><meta property="og:type" content="website"/><meta property="og:image" content="/site/assets/thumbnail.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="next-head-count" content="9"/><link rel="preload" href="/site/_next/static/css/e3c576235a6dc7ae.css" as="style"/><link rel="stylesheet" href="/site/_next/static/css/e3c576235a6dc7ae.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/site/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/site/_next/static/chunks/webpack-0aa8f7d9c8ed38d3.js" defer=""></script><script src="/site/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/site/_next/static/chunks/main-497a7a2a56968110.js" defer=""></script><script src="/site/_next/static/chunks/pages/_app-a74e9fe927d31b56.js" defer=""></script><script src="/site/_next/static/chunks/78e521c3-4ade3a605555334b.js" defer=""></script><script src="/site/_next/static/chunks/301-2088252f6a6ca1a0.js" defer=""></script><script src="/site/_next/static/chunks/pages/%5Bid%5D-8ae3107564787b9c.js" defer=""></script><script src="/site/_next/static/ZXh1UXrQVbnHUvbgtEFBp/_buildManifest.js" defer=""></script><script src="/site/_next/static/ZXh1UXrQVbnHUvbgtEFBp/_ssgManifest.js" defer=""></script><script src="/site/_next/static/ZXh1UXrQVbnHUvbgtEFBp/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div><nav class="bg-white md:shadow"><div class="max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex"><div class="flex-shrink-0 flex items-center cursor-pointer"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2732%27%20height=%2732%27/%3e"/></span><img alt="logo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="logo" srcSet="/site/assets/logo.png 1x, /site/assets/logo.png 2x" src="/site/assets/logo.png" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></div><div class="hidden sm:ml-6 sm:flex sm:space-x-8"><a class="text-indigo-700 hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/site/home">ImageNet-X</a><a class=" hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/site/dataset">Dataset</a><a class=" hover:text-indigo-500 text-gray-900 inline-flex items-center px-1 pt-1 text-sm font-semibold" href="/site/paper">Paper</a></div></div><div class="hidden sm:ml-6 sm:flex sm:space-x-8"><a href="https://github.com/facebookresearch/imagenetx/" class="text-gray-900 hover:text-indigo-500 ml-6 inline-flex items-center px-1 pt-1 text-sm font-semibold" target="_blank">Github<!-- --> <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M16.004 9.414l-8.607 8.607-1.414-1.414L14.589 8H7.004V6h11v11h-2V9.414z"></path></g></svg></a></div><div class="-mr-2 flex items-center sm:hidden"><button class="inline-flex items-center justify-center p-2 rounded-md text-gray-900" id="headlessui-disclosure-button-undefined" type="button" aria-expanded="false"><span class="sr-only">Open main menu</span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="block h-6 w-6" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 4h18v2H3V4zm0 7h18v2H3v-2zm0 7h18v2H3v-2z"></path></g></svg></button></div></div></div></nav><div class="prose prose-starter"><div class="comp_content flex w-screen justify-center self-stretch bg-indigo-200 text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-20 pb-4"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h1><code>ImageNet-X</code></h1><h3>Understanding model mistakes with human annotations of ImageNet.</h3><p><code>ImageNet-X</code> is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and
a random subset of 12,000 training samples.</p><span class="comp_button inline-block"><a class="flex rounded-md no-underline items-center justify-center font-semibold text-white hover:text-purple-800 hover:shadow-md bg-purple-800 hover:bg-white px-4 py-2 text-sm" href="/site/home"><p>Paper</p></a></span></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_gallery flex w-screen justify-center self-stretch bg-white text-gray-700"><div class="max-w-screen-xl flex flex-1 flex-row items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-4"><div class="flex flex-1 flex-grow-4 self-start max-w-none "><div class="p-1 aspect-auto"><div class="relative"><img src="/site/assets/images/cows_edited.png" class="comp_image object-cover w-full h-full m-0"/></div></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Annotation Form</h2><div class="relative"><img src="/site/assets/images/annotation_form.png" alt="ImageNet-X Annotation Form" class="comp_image object-cover w-full h-full m-0"/></div></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700">ImageNet-X can be used to surface mistake types for any ImageNet classifier</div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Using ImageNet-X</h2><ol>
<li>Git clone our repo</li>
</ol><pre><code>git clone url
</code></pre><ol start="2">
<li>Run plot.py</li>
<li>[step]</li>
</ol><p>[add collab link]</p><p>Example <a href="/site/assets/ViT_dashboard.html">robustness by metaclass dashboard for ViT</a></p><p>To evaluate mistake types for a new model, see below.</p></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-4 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Research abstract</h2><p>Deep learning vision systems are widely deployed across applications where reliability is critical.
However, even today’s best models can fail to recognize an object when its pose, lighting, or background varies.
While existing benchmarks surface examples that are challenging for models, they do not explain why such mistakes arise.</p><p>To address this need, we introduce ImageNet-X–a set of sixteen human annotations of factors such as pose, background, or lighting for the entire ImageNet1k validation set as well as a random subset of 12k training images.
Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model’s
(1) architecture – e.g. transformer vs. convolutional –, (2) learning paradigm – e.g. supervised vs. self-supervised –, and (3) training procedures – e.g. data augmentation.</p><p>Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories.
We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors.
For example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose.
Together, these insights suggests that to advance the robustness of modern vision models, future research should focus on collecting additional diverse data and understanding data augmentation schemes.
Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems make</p></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-red-100 text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-20 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Read the paper</h2><span class="comp_button inline-block"><a class="flex rounded-md no-underline items-center justify-center font-semibold text-white hover:text-purple-800 hover:shadow-md bg-purple-800 hover:bg-white px-4 py-2 text-sm" href="/site/home"><p>Paper</p></a></span></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_content flex w-screen justify-center self-stretch bg-white text-gray-700 "><div class="flex flex-1 flex-row box-border max-w-screen-xl items-center justify-start px-5 md:px-20 xl:px-10 pt-20 pb-20"><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 items-start justify-start h-full text-sm"><div class="opacity-70 text-gray-700"></div></div><div class="flex-1 flex-grow-4 self-start max-w-none prose-lg mx-4 text-gray-700"><h2>Using ImageNet-X for a new model</h2><p>See README on how to measure the mistake types on ImageNet</p><p>When you are ready to go live, run this command in the terminal to create a production build.</p><pre><code>pip install [something]
</code></pre><p>Take a look at the <a href="./release"><strong>Release Checklist</strong></a> section for deployment tips.</p></div><div class="hidden md:flex flex-1 flex-col ml-4 pt-2 items-start justify-start h-full text-sm"><div class="flex opacity-70 text-gray-700"></div></div></div></div>
<div class="comp_footer flex w-screen justify-center self-stretch bg-slate-200 text-undefined"><div class="flex flex-1 flex-row box-border max-w-screen-xl items-start justify-start px-5 md:px-20 xl:px-10 py-20 "><div class="hidden md:flex flex-1 flex-col mr-4 pt-2"></div><div class="flex flex-1 box-border pr-2 item-start justify-start text-sm"><div><p>ImageNet-X</p></div></div><div class="flex flex-1 box-border pr-2 item-start justify-start text-sm"><div><p><a href="https://github.com/fairinternal/image-variation-categories">Github repo</a></p></div></div><div class="hidden md:flex flex-1 flex-col mr-4 pt-2 "></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"home","mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      h1: \"h1\",\n      code: \"code\",\n      h3: \"h3\",\n      p: \"p\",\n      h2: \"h2\",\n      ol: \"ol\",\n      li: \"li\",\n      pre: \"pre\",\n      a: \"a\",\n      strong: \"strong\"\n    }, _provideComponents(), props.components), {Content, Button, Gallery, Image, Footer} = _components;\n    if (!Button) _missingMdxReference(\"Button\", true);\n    if (!Content) _missingMdxReference(\"Content\", true);\n    if (!Footer) _missingMdxReference(\"Footer\", true);\n    if (!Gallery) _missingMdxReference(\"Gallery\", true);\n    if (!Image) _missingMdxReference(\"Image\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsxs(Content, {\n        color: \"indigo-200\",\n        spaceTop: true,\n        children: [_jsx(_components.h1, {\n          children: _jsx(_components.code, {\n            children: \"ImageNet-X\"\n          })\n        }), _jsx(_components.h3, {\n          children: \"Understanding model mistakes with human annotations of ImageNet.\"\n        }), _jsxs(_components.p, {\n          children: [_jsx(_components.code, {\n            children: \"ImageNet-X\"\n          }), \" is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and\\na random subset of 12,000 training samples.\"]\n        }), _jsx(Button, {\n          url: \"/home\",\n          color: \"purple-800\",\n          small: true,\n          children: _jsx(_components.p, {\n            children: \"Paper\"\n          })\n        })]\n      }), \"\\n\", _jsx(Gallery, {\n        color: \"white\",\n        fullWidth: true,\n        children: _jsx(Image, {\n          url: \"/assets/images/cows_edited.png\"\n        })\n      }), \"\\n\", _jsxs(Content, {\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Annotation Form\"\n        }), _jsx(Image, {\n          url: \"/assets/images/annotation_form.png\",\n          caption: \"ImageNet-X Annotation Form\"\n        })]\n      }), \"\\n\", _jsxs(Content, {\n        noteLeft: \"ImageNet-X can be used to surface mistake types for any ImageNet classifier\",\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Using ImageNet-X\"\n        }), _jsxs(_components.ol, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Git clone our repo\"\n          }), \"\\n\"]\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            children: \"git clone url\\n\"\n          })\n        }), _jsxs(_components.ol, {\n          start: \"2\",\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Run plot.py\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"[step]\"\n          }), \"\\n\"]\n        }), _jsx(_components.p, {\n          children: \"[add collab link]\"\n        }), _jsxs(_components.p, {\n          children: [\"Example \", _jsx(_components.a, {\n            href: \"/site/assets/ViT_dashboard.html\",\n            children: \"robustness by metaclass dashboard for ViT\"\n          })]\n        }), _jsx(_components.p, {\n          children: \"To evaluate mistake types for a new model, see below.\"\n        })]\n      }), \"\\n\", _jsxs(Content, {\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Research abstract\"\n        }), _jsx(_components.p, {\n          children: \"Deep learning vision systems are widely deployed across applications where reliability is critical.\\nHowever, even today’s best models can fail to recognize an object when its pose, lighting, or background varies.\\nWhile existing benchmarks surface examples that are challenging for models, they do not explain why such mistakes arise.\"\n        }), _jsx(_components.p, {\n          children: \"To address this need, we introduce ImageNet-X–a set of sixteen human annotations of factors such as pose, background, or lighting for the entire ImageNet1k validation set as well as a random subset of 12k training images.\\nEquipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model’s\\n(1) architecture – e.g. transformer vs. convolutional –, (2) learning paradigm – e.g. supervised vs. self-supervised –, and (3) training procedures – e.g. data augmentation.\"\n        }), _jsx(_components.p, {\n          children: \"Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories.\\nWe also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors.\\nFor example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose.\\nTogether, these insights suggests that to advance the robustness of modern vision models, future research should focus on collecting additional diverse data and understanding data augmentation schemes.\\nAlong with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes the image recognition systems make\"\n        })]\n      }), \"\\n\", _jsxs(Content, {\n        color: \"red-100\",\n        spaceTop: true,\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Read the paper\"\n        }), _jsx(Button, {\n          url: \"/home\",\n          color: \"purple-800\",\n          small: true,\n          children: _jsx(_components.p, {\n            children: \"Paper\"\n          })\n        })]\n      }), \"\\n\", _jsxs(Content, {\n        spaceTop: true,\n        spaceBottom: true,\n        children: [_jsx(_components.h2, {\n          children: \"Using ImageNet-X for a new model\"\n        }), _jsx(_components.p, {\n          children: \"See README on how to measure the mistake types on ImageNet\"\n        }), _jsx(_components.p, {\n          children: \"When you are ready to go live, run this command in the terminal to create a production build.\"\n        }), _jsx(_components.pre, {\n          children: _jsx(_components.code, {\n            children: \"pip install [something]\\n\"\n          })\n        }), _jsxs(_components.p, {\n          children: [\"Take a look at the \", _jsx(_components.a, {\n            href: \"./release\",\n            children: _jsx(_components.strong, {\n              children: \"Release Checklist\"\n            })\n          }), \" section for deployment tips.\"]\n        })]\n      }), \"\\n\", _jsxs(Footer, {\n        columns: true,\n        children: [_jsx(\"div\", {\n          children: _jsx(_components.p, {\n            children: \"ImageNet-X\"\n          })\n        }), _jsx(\"div\", {\n          children: _jsx(_components.p, {\n            children: _jsx(_components.a, {\n              href: \"https://github.com/fairinternal/image-variation-categories\",\n              children: \"Github repo\"\n            })\n          })\n        })]\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"currentMetaData":{"title":"ImageNet-X","order":0,"id":"home"},"allMetaData":[{"id":"home","title":"ImageNet-X","order":0},{"id":"dataset","title":"Dataset","order":1},{"id":"paper","title":"Paper","order":2}]},"__N_SSG":true},"page":"/[id]","query":{"id":"home"},"buildId":"ZXh1UXrQVbnHUvbgtEFBp","assetPrefix":"/site","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>